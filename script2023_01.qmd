---
title: "PMP 40ans"
author: "CB MB SB"
format: html
subtitle: "Compter les mots" 
title-block-banner: img01.jpeg
date: today
execute : 
  message: false
  warning: false
code-fold: true
editor: visual
---

## Running Code

```{r 01}
library(tidyverse) #on ne peut plus s'en passer
library(quanteda) # les bases du nlp
library(quanteda.textstats)
library(quanteda.textmodels)
library(scales)
library(ggwordcloud)
library(spacyr)
library(readr) #lire les donnees
library(stringr) #manipuler le texte
library(pals)
library(ape) #pour de jolis dendogrammes
library(tidytext) #du nlp
library(proustr) #un package sympa pour les dicos
library(mixr)
library(widyr)
library(igraph) #classic des reseaux
library(ggrepel) #gestion des labels
library(ggraph) 
library(gglorenz) #courbe de concentration
library('extrafont') #c'est pour igraph
# Import des fontes du système - cela peut être long
#font_import()
#fonts() # Liste des polices disponibles
#loadfonts(device = "pdf") # Indiquer device = "pdf" pour produire un pdf 

theme_set(theme_minimal())

col<- c("#ca7dcc","#fb6a4a","orange","#67000d", "firebrick", "pink", "ivory3","skyblue")

```

## Data

```{r 02}
PMP <- read_csv("./data/PMPLast.csv")

dn<-PMP %>% 
  select(Key,`Publication Year`,Title, `Abstract Note`, Issue, Volume)%>% 
  rename(Year=`Publication Year`, Abstract=`Abstract Note`) %>% 
  mutate(text=paste0(Title,". ", Abstract),
         nchar=nchar(text),
         n_words = stringr::str_count(text, ' ')+1,
         decade=as.character(ifelse(Year<2000,(floor((Year-1900)/10)*10)+1900,(floor((Year-2000)/10)*10)+2000)),
         Issue2=ifelse(str_sub(Issue, 1,3)=="Vol",str_sub(Issue, -1),Issue), 
         Issue2=ifelse(Issue2=="1-2",1, ifelse(Issue2=="3-4",3, Issue2)),
         x=as.numeric(Issue2)/4,
         numéro=Year+x) %>%  
  filter(Abstract!="Null") #critere du résume

ggplot(dn, aes(x=nchar))+geom_histogram(binwidth = 50)+theme_minimal()
ggplot(dn, aes(x=n_words))+geom_histogram(binwidth = 20)+theme_minimal()
ggplot(dn, aes(x=n_words ,y=nchar/n_words))+
  geom_point()+
  theme_minimal()+geom_smooth()+xlim(0, 750)+
  labs(x="nombre de mots",
       y="longueur des mots")
```

C'est assez stable

```{r 03}

dn_year<-dn %>% 
  group_by(Year) %>% 
  summarise(n=n(),n_words=sum(n_words))

ggplot(dn_year, aes(x=Year,y=n))+
  geom_smooth(method = "loess",span=.35, color="Coral2")+
  labs(title="Evolution du nombre d'articles par an",
       y="nombre de papiers",
       x="Année de publication",caption="corpus PMP")+
  scale_x_continuous(breaks=c(1980, 1985, 1990,1995,2000,2005,2010, 2015, 2020))+
  ylim(0,40)

ggsave("./images/evol0.jpeg", width = 27, height = 18, units = "cm")

```

Un tournant en 2005 ? La difficulté à alimenter la revue ?

```{r 04}

ggplot(dn_year, aes(x=Year,y=n_words/n))+
  geom_smooth(method = "loess",span=.3,color="darkgreen")+
  labs(title="Evolution du nombre moyen de mots par abstract",
       y="nombre moyen de mots",
       x="Année de publication",caption="corpus PMP")

ggsave("./images/evol1.jpeg", width = 27, height = 18, units = "cm")

```

## Le lexique

en tfidf pour pondérer la généralité. Certains mots se retrouvent dans tous les textes, s'ils sont fréquents ils sont aussi très peu distinctifs.

Avec les collocations parce que ce type de langage favorise les expressions composées : par exemple "service public"

```{r 06}
corp<-corpus(dn$text, docvars =dn)
toks <- tokens(corp, remove_punct = TRUE, padding=FALSE)%>% 
 tokens_select(pattern = stopwords('french'), selection = 'remove')

tstat_col_caps <- 
  tokens_select(toks, case_insensitive = FALSE,
                                padding = FALSE) %>% 
           textstat_collocations(min_count = 5, size=2:5)%>%
  filter(lambda>8 & z>4)

head(tstat_col_caps, 10)

toks_comp <- tokens_compound(toks, pattern = tstat_col_caps) 

dfm <-  dfm(toks_comp)

tstat_freq <- as.data.frame(textstat_frequency(dfm))

N<-as.numeric(nrow(tstat_freq))
M=sum(tstat_freq$frequency)

tstat_freq2<-tstat_freq %>% 
  mutate(idf=log(N/docfreq),
         tfidf=(frequency/M)*idf)%>%
  filter(rank>0 & rank<400)

set.seed(42)

ggplot(tstat_freq2, aes(label = feature, size = tfidf)) +
  geom_text_wordcloud() +
  scale_size_area(max_size = 5) +
  scale_color_discrete()+
  labs(title="les 400 mots les plus fréquents dans les titres et résumés des articles de PMP",
       subtitle = "les police sont proportionnele au tfidf de chaque terme",
       caption="PMP data")

ggsave("./images/lexic2.jpeg", width = 27, height = 18, units = "cm")

```

Par groupe (decade)

```{r 07}

quant_deca<- toks_comp %>% 
  tokens_group(groups = decade)
  
dfm<- quant_deca %>%dfm() %>% 
  dfm_group(groups=decade)

tstat_freq <- as.data.frame(textstat_frequency(dfm, groups = decade))

N<-as.numeric(nrow(tstat_freq))
M=sum(tstat_freq$frequency)

tstat_freq2<-tstat_freq %>% 
  mutate(idf=log(N/docfreq),
         tfidf=(frequency/M)*idf) %>%
  filter(tfidf>0.004 & tfidf<0.0112)

set.seed(42)

ggplot(tstat_freq2, aes(label = feature, size = tfidf)) +
  geom_text_wordcloud(aes(color=tfidf)) +
  scale_size_area(max_size = 3) +
  labs(title="les mots les plus fréquents dans les titres des articles de PMP")+
  facet_wrap(vars(group))+  scale_color_gradient(low="#fb6a4a", high="#67000d")

ggsave("./images/keyword0.jpeg", width = 27, height = 18, units = "cm")

```

## Comptons les mots ?

## Une petite ontologie :

L'angle d'analyse

-   Politique publique
-   science administrative
-   Gestion et Management

Les niveaux de gouvernance \* Institutions internationale \* Etat \* Collectivité locales et communales

Les champs : \* défense \* police \* justice \* Santé \* Education \* recherche \* protection sociale \* retraites \* développement économique \*

Les problèmes : \* modernisation \* réforme \* efficacité/ performance \* adaptation

### politique et management

```{r 08}

dn$PolitiquePublique<-str_count(dn$text, pattern = "[P|p]olitique.*\\s[P|p]ublique.*")/dn$n_words
dn$Gestion<-str_count(dn$text, pattern = "[G|g]estion")/dn$n_words
dn$Management<-str_count(dn$text, pattern = "[M|m]anagement")/dn$n_words
dn$ServicePublic<-str_count(dn$text, pattern = "[S|s]ervice.*[:blank:][P|p]ubli.*")/dn$n_words

foo<-dn %>% 
  group_by(Year)%>% 
  summarise(PolitiquePublique=mean(PolitiquePublique),
            Gestion=mean(Gestion),
            Management=mean(Management),
            ServicePublic=mean(ServicePublic),

            ) %>%
  pivot_longer(-Year, names_to = "keyword",values_to = "n")

ggplot(foo, aes(x=Year, y=n, group=keyword))+
  geom_smooth(method="loess", alpha=0.1,span=0.5,aes(color=keyword))+
  scale_color_manual(values = col)+
  scale_y_continuous(labels=scales::percent,limits=c(0, NA))+
  labs(y="densité", 
       x=NULL, 
       title = "Fréquence des termes",
       subtitle = "",
       caption="PMP data")

ggsave("./images/keyword1.jpeg", width = 27, height = 18, units = "cm")

```

### Institutions

```{r 09}

dn$Collectivite<-str_count(dn$text, pattern = "[C|c]ollectivit.*")/dn$n_words
dn$Hopital<-str_count(dn$text, pattern = "[H|h][ô|o][.|s]pital.*")/dn$n_words
dn$Etat<-str_count(dn$text, pattern = "[E|é]tat")/dn$n_words
dn$Europe<-str_count(dn$text, pattern = "[E|e]urop.*")/dn$n_words
dn$Université<-str_count(dn$text,"[U|u]niversit.*")/dn$n_words
dn$Territoire<-str_count(dn$text,"[T|t]errit[o, oi]r.*")/dn$n_words
dn$Administration<-str_count(dn$text,"[A|a]dministrat.*")/dn$n_words
dn$Commune<-str_count(dn$text,".*[C|c]ommun.*")/dn$n_words

foo<-dn %>% 
  group_by(Year)%>% 
  summarise(Université=mean(Université),
            Collectivite=mean(Collectivite),
            Hopital=mean(Hopital),
            Etat=mean(Etat),
            Europe=mean(Europe),
            Territoire=mean(Territoire),
            Administration=mean(Administration),
            Commune=mean(Commune)
                        ) %>%
  pivot_longer(-Year, names_to = "keyword",values_to = "n")

ggplot(foo, aes(x=Year, y=n, group=keyword))+
  geom_smooth(method="loess", alpha=0.1,span=0.45,aes(color=keyword))+
  scale_color_manual(values = col)+
  scale_y_continuous(labels=scales::percent, limits=c(0, NA))+
  labs(y="densité", x=NULL, title = "Institution")

ggsave("./images/keyword2.jpeg", width = 27, height = 18, units = "cm")
```

# la performance et sa mesure

```{r 10}


#performance

dn$Performance<-str_count(dn$text,"[P|p]erform.*")/dn$n_words
dn$Rentabilité<-str_count(dn$text,"[R|r]entab.*")/dn$n_words
dn$Efficacité<-str_count(dn$text,"[E|e]fficac.*")/dn$n_words
#dn$Efficience<-str_count(dn$text,"[E|e]effici.*")/dn$n_words
dn$Rentabilité<-str_count(dn$text,"[R|r]entab.*")/dn$n_words
dn$Evaluation<-str_count(dn$text,"[E|é]valu.*")/dn$n_words
dn$Controle<-str_count(dn$text,"[C|c]ontrôl.*")/dn$n_words
dn$Reforme<-str_count(dn$text,"[R|r]éform.*")/dn$n_words
dn$Modernisation<-str_count(dn$text,"[M|m]odern.*")/dn$n_words


foo<-dn %>%
  select(Key,Year,Performance, Rentabilité, Efficacité, #Efficience, 
         Rentabilité, Evaluation, Controle, Reforme, Modernisation )%>% 
  pivot_longer(-c(Key, Year), names_to = "variable", values_to = "taux") %>%
  group_by(Year, variable)%>%
  summarise(taux=mean(taux))  %>%
  ungroup()
              
            
foo1<-foo %>%
  pivot_wider(id_cols =Year, names_from = "variable", values_from = "taux") %>%
  select(-Year)

ggplot(foo,aes(x=Year,y=taux,group=variable))+
#  geom_line(stat="identity", aes(color=variable), linewidth=1)+
  geom_smooth(aes(color=variable), span=.45, alpha=.1)+
  scale_y_continuous(labels=scales::percent, limits=c(0,0.0025))+
  labs(y="densité", x=NULL, title = "Performance")+
    scale_color_manual(values = col)


ggsave("./images/keyword3.jpeg", width = 27, height = 18, units = "cm")

```

## corrélations entre les keywords

```{r 11}

foo<-dn %>%
  select(Key,Year,Performance, Rentabilité, Efficacité, #Efficience, 
         Rentabilité, Evaluation, Controle, Commune, ServicePublic,
         Collectivite, Etat, Université, Europe, Territoire,Administration,
         Management, PolitiquePublique, Management, Modernisation, Reforme)%>% 
  pivot_longer(-c(Key, Year), names_to = "variable", values_to = "taux") %>%
  group_by(Year, variable)%>%
  summarise(taux=mean(taux)) %>%
  group_by(variable)%>%  
  mutate(delta = c(0,diff(taux))) %>% 
  select(-taux)%>%
  ungroup()%>% 
  pivot_wider(Year, names_from = "variable", values_from = "delta") %>% 
  select(-Year)


r <-cor(foo) 

library(ggcorrplot)

ggcorrplot(r, hc.order = TRUE, type = "lower",
   outline.col = "white",
   colors = c("#6D9EC1", "white", "#E46726"), lab=TRUE, lab_size=2)

ggsave("./images/keyword_cor.jpeg", width = 27, height = 18, units = "cm")

library("FactoMineR")
res.pca <- PCA(r, ncp=6,graph = FALSE)

library("factoextra")
eig.val <- get_eigenvalue(res.pca)
eig.val
fviz_pca_var(res.pca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Évite le chevauchement de texte
             )
fviz_pca_var(res.pca, axes= c(3,4),col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Évite le chevauchement de texte
             )

library(psych)
mle <- fa(r,6,rotate="oblimin",fm="uls")
mle
summary(mle)

fa.sort(mle,polar=FALSE)


```
