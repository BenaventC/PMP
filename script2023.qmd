---
title: "PMP 40ans"
author: "CB MB SB"
format: html
subtitle: "Compter les mots" 
title-block-banner: img01.jpeg
date: today
execute : 
  message: false
  warning: false
code-fold: true
editor: visual
---

## Running Code

```{r 01}
library(tidyverse) #on ne peut plus s'en passer
library(quanteda) # les bases du nlp
library(quanteda.textstats)
library(quanteda.textmodels)
library(scales)
library(ggwordcloud)
library(spacyr)
library(readr) #lire les donnees
library(stringr) #manipuler le texte
library(pals)
library(ape) #pour de jolis dendogrammes
library(tidytext) #du nlp
library(proustr) #un package sympa pour les dicos
library(mixr)
library(widyr)
library(igraph) #classic des reseaux
library(ggrepel) #gestion des labels
library(ggraph) 
library(gglorenz) #courbe de concentration
library('extrafont') #c'est pour igraph
# Import des fontes du système - cela peut être long
#font_import()
#fonts() # Liste des polices disponibles
#loadfonts(device = "pdf") # Indiquer device = "pdf" pour produire un pdf 

theme_set(theme_minimal())

col<- c("#ca7dcc","#fb6a4a","orange","#67000d", "firebrick", "pink", "ivory3","skyblue")

```

## Data

```{r 02}
PMP <- read_csv("./data/PMPLast.csv")

dn<-PMP %>% 
  select(Key,`Publication Year`,Title, `Abstract Note`, Issue, Volume)%>% 
  rename(Year=`Publication Year`, Abstract=`Abstract Note`) %>% 
  mutate(text=paste0(Title,". ", Abstract),
         nchar=nchar(text),
         n_words = stringr::str_count(text, ' ')+1,
         decade=as.character(ifelse(Year<2000,(floor((Year-1900)/10)*10)+1900,(floor((Year-2000)/10)*10)+2000)),
         Issue2=ifelse(str_sub(Issue, 1,3)=="Vol",str_sub(Issue, -1),Issue), 
         Issue2=ifelse(Issue2=="1-2",1, ifelse(Issue2=="3-4",3, Issue2)),
         x=as.numeric(Issue2)/4,
         numéro=Year+x) %>%  
  filter(Abstract!="Null") #critere du résume

ggplot(dn, aes(x=nchar))+geom_histogram(binwidth = 50)+theme_minimal()
ggplot(dn, aes(x=n_words))+geom_histogram(binwidth = 20)+theme_minimal()
ggplot(dn, aes(x=n_words ,y=nchar/n_words))+
  geom_point()+
  theme_minimal()+geom_smooth()+xlim(0, 750)+
  labs(x="nombre de mots",
       y="longueur des mots")
```

C'est assez stable

```{r 03}

dn_year<-dn %>% 
  group_by(Year) %>% 
  summarise(n=n(),n_words=sum(n_words))

ggplot(dn_year, aes(x=Year,y=n))+
  geom_smooth(method = "loess",span=.35, color="Coral2")+
  labs(title="Evolution du nombre d'articles par an",
       y="nombre de papiers",
       x="Année de publication",caption="corpus PMP")+
  scale_x_continuous(breaks=c(1980, 1985, 1990,1995,2000,2005,2010, 2015, 2020))+
  ylim(0,40)

ggsave("./images/evol0.jpeg", width = 27, height = 18, units = "cm")

```

Un tournant en 2005 ? La difficulté à alimenter la revue ?

```{r 04}

ggplot(dn_year, aes(x=Year,y=n_words/n))+
  geom_smooth(method = "loess",span=.3,color="darkgreen")+
  labs(title="Evolution du nombre moyen de mots par abstract",
       y="nombre moyen de mots",
       x="Année de publication",caption="corpus PMP")

ggsave("./images/evol1.jpeg", width = 27, height = 18, units = "cm")

```

## Le lexique

en tf idf

Avec les collocations parce que ce type de langage favorise les expressions composées : par exemple "taux d'intérêt"

```{r 06}
corp<-corpus(dn$text, docvars =dn)
toks <- tokens(corp, remove_punct = TRUE, padding=FALSE)%>% 
 tokens_select(pattern = stopwords('french'), selection = 'remove')

tstat_col_caps <- 
  tokens_select(toks, case_insensitive = FALSE,
                                padding = FALSE) %>% 
           textstat_collocations(min_count = 5, size=2:5)%>%
  filter(lambda>8 & z>4)

head(tstat_col_caps, 10)

toks_comp <- tokens_compound(toks, pattern = tstat_col_caps) 

dfm <-  dfm(toks_comp)

tstat_freq <- as.data.frame(textstat_frequency(dfm))

N<-as.numeric(nrow(tstat_freq))
M=sum(tstat_freq$frequency)

tstat_freq2<-tstat_freq %>% 
  mutate(idf=log(N/docfreq),
         tfidf=(frequency/M)*idf)%>%
  filter(rank>0 & rank<400)

set.seed(42)

ggplot(tstat_freq2, aes(label = feature, size = tfidf)) +
  geom_text_wordcloud() +
  scale_size_area(max_size = 5) +
  scale_color_discrete()+
  labs(title="les 400 mots les plus fréquents dans les titres et résumés des articles de PMP",
       subtitle = "les police sont proportionnele au tfidf de chaque terme",
       caption="PMP data")

ggsave("./images/lexic2.jpeg", width = 27, height = 18, units = "cm")

```

Par groupe (decade)

```{r 07}

quant_deca<- toks_comp %>% 
  tokens_group(groups = decade)
  
dfm<- quant_deca %>%dfm() %>% 
  dfm_group(groups=decade)

tstat_freq <- as.data.frame(textstat_frequency(dfm, groups = decade))

N<-as.numeric(nrow(tstat_freq))
M=sum(tstat_freq$frequency)

tstat_freq2<-tstat_freq %>% 
  mutate(idf=log(N/docfreq),
         tfidf=(frequency/M)*idf) %>%
  filter(tfidf>0.004 & tfidf<0.0112)

set.seed(42)

ggplot(tstat_freq2, aes(label = feature, size = tfidf)) +
  geom_text_wordcloud(aes(color=tfidf)) +
  scale_size_area(max_size = 3) +
  labs(title="les mots les plus fréquents dans les titres des articles de PMP")+
  facet_wrap(vars(group))+  scale_color_gradient(low="#fb6a4a", high="#67000d")

ggsave("./images/keyword0.jpeg", width = 27, height = 18, units = "cm")

```

## Comptons les mots ?

### politique et management

```{r 08}

dn$keyword1<-str_count(dn$text, pattern = "[P|p]olitique.*\\s[P|p]ublique.*")/dn$n_words
dn$keyword2<-str_count(dn$text, pattern = "[G|g]estion")/dn$n_words
dn$keyword3<-str_count(dn$text, pattern = "[M|m]anagement")/dn$n_words

foo<-dn %>% 
  group_by(Year)%>% 
  summarise(PolitiquePublique=mean(keyword1),
            Gestion=mean(keyword2),
            Management=mean(keyword3)) %>%
  pivot_longer(-Year, names_to = "keyword",values_to = "n")

ggplot(foo, aes(x=Year, y=n, group=keyword))+
  geom_smooth(method="loess", alpha=0.1,span=0.5,aes(color=keyword))+
  scale_color_manual(values = col)+
  scale_y_continuous(labels=scales::percent)+
  labs(y="densité", 
       x=NULL, 
       title = "Fréquence des termes",
       subtitle = "",
       caption="PMP data")

ggsave("./images/keyword1.jpeg", width = 27, height = 18, units = "cm")

```

### Institution

```{r 09}

dn$Collectivite<-str_count(dn$text, pattern = "[C|c]ollectivit.*")/dn$n_words
dn$Hopital<-str_count(dn$text, pattern = "[H|h][ô|o][.|s]pital.*")/dn$n_words
dn$Etat<-str_count(dn$text, pattern = "Etat")/dn$n_words
dn$Europe<-str_count(dn$text, pattern = "[E|e]urop.*")/dn$n_words
dn$Université<-str_count(dn$text,"[U|u]niversit.*")/dn$n_words
dn$Territoire<-str_count(dn$text,"[T|t]errit[o, oi]r.*")/dn$n_words
dn$Administration<-str_count(dn$text,"[A|a]dministrat.*")/dn$n_words

foo<-dn %>% 
  group_by(Year)%>% 
  summarise(Université=mean(Université),
            Collectivite=mean(Collectivite),
            Hopital=mean(Hopital),
            Etat=mean(Etat),
            Europe=mean(Europe),
            Territoire=mean(Territoire),
            Administration=mean(Administration)
                        ) %>%
  pivot_longer(-Year, names_to = "keyword",values_to = "n")

ggplot(foo, aes(x=Year, y=n, group=keyword))+
  geom_smooth(method="loess", alpha=0.1,span=0.45,aes(color=keyword))+
  scale_color_manual(values = col)+
  scale_y_continuous(labels=scales::percent, limits=c(0, NA))+
  labs(y="densité", x=NULL, title = "Performance")

ggsave("./images/keyword2.jpeg", width = 27, height = 18, units = "cm")
```


# la performance et sa mesure


```{r 10}


#performance

dn$Performance<-str_count(dn$text,"[P|p]erform.*")/dn$n_words
dn$Rentabilité<-str_count(dn$text,"[R|r]entab.*")/dn$n_words
dn$Efficacité<-str_count(dn$text,"[E|e]fficac.*")/dn$n_words
#dn$Efficience<-str_count(dn$text,"[E|e]effici.*")/dn$n_words
dn$Rentabilité<-str_count(dn$text,"[R|r]entab.*")/dn$n_words
dn$Evaluation<-str_count(dn$text,"[E|é]valu.*")/dn$n_words
dn$Controle<-str_count(dn$text,"[C|c]ontrôl.*")/dn$n_words


foo<-dn %>%
  select(Key,Year,Performance, Rentabilité, Efficacité, #Efficience, 
         Rentabilité, Evaluation, Controle)%>% 
  pivot_longer(-c(Key, Year), names_to = "variable", values_to = "taux") %>%
  group_by(Year, variable)%>%
  summarise(taux=mean(taux))  %>%
  ungroup()
              
            
foo1<-foo %>%
  pivot_wider(id_cols =Year, names_from = "variable", values_from = "taux") %>%
  select(-Year)

ggplot(foo,aes(x=Year,y=taux,group=variable))+
#  geom_line(stat="identity", aes(color=variable), linewidth=1)+
  geom_smooth(aes(color=variable), span=.45, alpha=.1)+
  scale_y_continuous(labels=scales::percent, limits=c(0,0.0025))+
  labs(y="densité", x=NULL, title = "Performance")+
    scale_color_manual(values = col)


ggsave("./images/keyword3.jpeg", width = 27, height = 18, units = "cm")


cor(foo1)


```
